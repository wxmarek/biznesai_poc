
# -*- coding: utf-8 -*-
"""gr4_gpt2_BiznesAI.ipynb

// GPU T4

Automatically generated by Colab.
"""

!pip install transformers datasets torch

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from datasets import Dataset
from google.colab import userdata, drive

drive.mount('/content/drive')
test_data_txt = "/content/drive/My Drive/Colab Notebooks/Data/model_tests_cleaned.txt"

# ---------------------------------------------
# 1. Ładowanie modelu i tokenizera GPT-2
# ---------------------------------------------
model_name = "gpt2"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

# Ustawiamy token PAD na eos (GPT-2 domyślnie go nie ma)
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# ---------------------------------------------
# 2. Funkcja do wczytania danych
# ---------------------------------------------
def load_data(file_path):
    with open(file_path, "r") as file:
        text = file.read().split("###")  # używamy '###' jako separatora
        text = [fragment.strip() for fragment in text if fragment.strip()]
    return text

# Ładujemy dane
scenariusze = load_data(test_data_txt)
print(f"Liczba wczytanych scenariuszy: {len(scenariusze)}")

# Tworzymy Dataset
dataset = Dataset.from_dict({"text": scenariusze})

# ---------------------------------------------
# 3. Podział: train / val / test
# ---------------------------------------------
train_test = dataset.train_test_split(test_size=0.2, seed=42)
train_val  = train_test["train"].train_test_split(test_size=0.125, seed=42)

train_dataset = train_val["train"]
val_dataset   = train_val["test"]
test_dataset  = train_test["test"]

print("Train size:", len(train_dataset))
print("Val size:  ", len(val_dataset))
print("Test size: ", len(test_dataset))

# ---------------------------------------------
# 4. Funkcja do tokenizacji
# ---------------------------------------------
def tokenize_function(examples):
    inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding='max_length',
        max_length=1024,
        add_special_tokens=True
    )
    # Dla GPT-2 (Causal LM) labels = input_ids
    inputs["labels"] = inputs["input_ids"].copy()
    return inputs

# Tokenizujemy zbiory
tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val   = val_dataset.map(tokenize_function, batched=True)
tokenized_test  = test_dataset.map(tokenize_function, batched=True)

# ---------------------------------------------
# 5. Konfiguracja treningu
# ---------------------------------------------
training_args = TrainingArguments(
    output_dir="./gpt2_finetuned_model",
    num_train_epochs=10,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy="epoch",       # Ewaluacja co epokę
    save_strategy="epoch",             # Zapis co epokę
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    load_best_model_at_end=True,
    report_to="none",                  # Brak integracji z W&B/TensorBoard
    fp16=torch.cuda.is_available(),
    #
    # Dodane parametry logowania (trening):
    #
    logging_strategy="epoch"           # Logowanie co epokę (na koniec)
    # Możesz wybrać:
    # logging_strategy="steps",
    # logging_steps=100  # przykładowo co 100 kroków
)

# ---------------------------------------------
# 6. Inicjalizacja Trainer
# ---------------------------------------------
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# ---------------------------------------------
# 7. Trening
# ---------------------------------------------
trainer.train()

# ---------------------------------------------
# 8. Zapis wytrenowanego modelu i tokenizera
# ---------------------------------------------
model.save_pretrained("./gpt2_finetuned_model")
tokenizer.save_pretrained("./gpt2_finetuned_model")

print("Model i tokenizer zostały zapisane w katalogu './gpt2_finetuned_model'.")

# --------------------------------------------------
# 9. Ewaluacja na zbiorze testowym
# --------------------------------------------------
test_metrics = trainer.evaluate(tokenized_test)
print("Wyniki na zbiorze testowym:", test_metrics)
print("eval_loss =", test_metrics.get("eval_loss", None))

# Opcjonalnie można przeliczyć eval_loss na perplexity:
import math
eval_loss = test_metrics.get("eval_loss")
if eval_loss is not None:
    perplexity = math.exp(eval_loss)
    print(f"Perplexity (zbiór testowy) = {perplexity:.2f}")

# --------------------------------------------------
# 10. Generowanie nowych scenariuszy
# --------------------------------------------------
def generate_scenario(
    prompt,
    max_new_tokens=100,
    num_return_sequences=1,
    temperature=0.7,
    top_k=50,
    top_p=0.9
):
    """
    Funkcja generuje nowe scenariusze na podstawie wytrenowanego modelu.
    :param prompt: wstępny tekst (początek scenariusza / tytuł / cokolwiek)
    :param max_new_tokens: ile tokenów dopisać (bez promptu)
    :param num_return_sequences: ile różnych wariantów wygenerować
    :param temperature: kontrola "kreatywności"
    :param top_k: ogranicza wybór kolejnego tokenu do k najbardziej prawdopodobnych
    :param top_p: nucleus sampling do p (sumaryczne prawdopodobieństwo)
    :return: lista wygenerowanych tekstów (stringów)
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        do_sample=True,
        num_return_sequences=num_return_sequences,
        pad_token_id=tokenizer.eos_token_id
    )

    scenarios = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]
    return scenarios

# Przykład użycia funkcji generowania
example_prompt = "Test Name: Advanced Probe Configuration\nSteps:"
generated_scenarios = generate_scenario(
    prompt=example_prompt,
    max_new_tokens=60,
    num_return_sequences=5,  # Wygeneruj 2 różnych kontynuacji
    temperature=0.7
)

for i, scenario in enumerate(generated_scenarios, start=1):
    print(f"\n--- Scenario {i} ---\n{scenario}\n")

