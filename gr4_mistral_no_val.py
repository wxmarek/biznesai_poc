# -*- coding: utf-8 -*-
"""gr4_mistral7b-instruct_BiznesAI.ipynb

// GPU A100

Automatically generated by Colab.
"""

!pip install transformers datasets torch

import torch
import gc
import os
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model
from huggingface_hub import login
from google.colab import userdata, drive

# ---------------------------------------------
# 1. Logowanie do Hugging Face
# ---------------------------------------------
HF_TOKEN = userdata.get('HF_TOKEN')
if HF_TOKEN:
    login(HF_TOKEN)
    print("Successfully logged in to Hugging Face!")
else:
    print("Token is not set. Please save the token first.")

# ---------------------------------------------
# 2. Montowanie Dysku Google i ustawienia ścieżek
# ---------------------------------------------
drive.mount('/content/drive')
test_data_txt = "/content/drive/My Drive/Colab Notebooks/Data/model_tests_cleaned.txt"

# ---------------------------------------------
# 3. Konfiguracja urządzenia i modelu
# ---------------------------------------------
model_name = "mistralai/Mistral-7B-Instruct-v0.1"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Sprawdzenie dostępności GPU
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Pamięć GPU zajęta: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    print(f"Maksymalnie przydzielona: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB")
else:
    print("GPU nie jest dostępne. Używasz CPU!")

# ---------------------------------------------
# 4. Ładowanie tokenizatora i modelu
# ---------------------------------------------
print("Ładowanie tokenizatora...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Ustawienie tokenu pad na eos

print(f"Ładowanie modelu na {device}...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,  # Zmniejszenie precyzji dla GPU
    device_map="auto"
)
model.config.pad_token_id = tokenizer.pad_token_id
print("Model załadowany poprawnie.")

# ---------------------------------------------
# 5. Wczytanie danych treningowych
# ---------------------------------------------
def load_data(file_path):
    with open(file_path, "r") as file:
        text = file.read().split("###")  # używamy '###' jako separatora
        text = [fragment.strip() for fragment in text if fragment.strip()]
    return text

scenariusze = load_data(test_data_txt)
train_dataset = Dataset.from_dict({"text": scenariusze})

# ---------------------------------------------
# 6. Tokenizacja danych
# ---------------------------------------------
def tokenize_function(examples):
    texts = examples["text"] if isinstance(examples["text"], list) else [examples["text"]]
    inputs = tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        max_length=512,
        add_special_tokens=True,
        return_tensors=None
    )
    inputs["labels"] = inputs["input_ids"].copy()
    return {key: value for key, value in inputs.items() if key in {"input_ids", "attention_mask", "labels"}}

tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
print(tokenized_train)

# ---------------------------------------------
# 7. Konfiguracja LoRA (Low-Rank Adaptation)
# ---------------------------------------------
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=["q_proj", "v_proj"]
)
model = get_peft_model(model, lora_config)

# ---------------------------------------------
# 8. Konfiguracja parametrów treningowych
# ---------------------------------------------
training_args = TrainingArguments(
    output_dir="./mistral_finetuned_model",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    report_to="none",
    fp16=True,
    logging_strategy="epoch",
    optim="adamw_torch",
    remove_unused_columns=False
)

print("✅ Model i argumenty treningowe skonfigurowane poprawnie.")

# ---------------------------------------------
# 9. Inicjalizacja Trainer
# ---------------------------------------------
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    data_collator=data_collator
)
print(trainer)

# ---------------------------------------------
# 10. Trening modelu
# ---------------------------------------------
trainer.train()

# ---------------------------------------------
# 11. Zapis wytrenowanego modelu i tokenizera
# ---------------------------------------------
model.save_pretrained("./mistral_finetuned_model")
tokenizer.save_pretrained("./mistral_finetuned_model")
print("Model i tokenizer zostały zapisane w katalogu './mistral_finetuned_model'.")

# ---------------------------------------------
# 12. Czyszczenie pamięci GPU
# ---------------------------------------------
torch.cuda.empty_cache()
torch.cuda.ipc_collect()
print("✅ Pamięć GPU została wyczyszczona.")

import torch
import gc
import os
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM
)

# ---------------------------------------------
# 1. Ładowanie wytrenowanego modelu i tokenizatora
# ---------------------------------------------
model_path = "./mistral_finetuned_model"
print("Ładowanie tokenizatora...")
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token  # Ustawienie pad_token na eos_token

print("Ładowanie modelu...")
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)
model.config.pad_token_id = tokenizer.pad_token_id  # Ustawienie pad_token_id
model.eval()
print("Model i tokenizer załadowane poprawnie.")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# ---------------------------------------------
# 2. Funkcja do generowania scenariusza testowego
# ---------------------------------------------
def generate_test_scenario(prompt, max_length=500):
    """Generuje scenariusz testowy na podstawie promptu."""
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(device)

    with torch.no_grad():
        output = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,  # Dodanie attention_mask
            max_length=max_length,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id  # Zapewnienie poprawnego wypełniania
        )

    return tokenizer.decode(output[0], skip_special_tokens=True)

# ---------------------------------------------
# 3. Promptowanie modelu i generowanie 3 scenariuszy
# ---------------------------------------------
def prompt_for_test_scenarios(num_scenarios=3):
    user_prompt = "Wygeneruj scenariusz testowy dla głowicy USG w formacie:\n\n"
    user_prompt += "Test Name: [Nazwa testu]\nSteps:\n1. [Krok 1]\n2. [Krok 2]\n3. [Krok 3]\n\nExpected Result:\n- [Oczekiwany wynik 1]\n- [Oczekiwany wynik 2]\n\n###\n"

    print("Generowanie scenariuszy testowych...")
    scenarios = [generate_test_scenario(user_prompt) for _ in range(num_scenarios)]

    for idx, scenario in enumerate(scenarios, 1):
        print(f"\nScenariusz {idx}:\n")
        print(scenario)
        print("###")

    return scenarios

# Generowanie 3 scenariuszy testowych
if __name__ == "__main__":
    generated_scenarios = prompt_for_test_scenarios()

# ---------------------------------------------
# 4. Czyszczenie pamięci GPU
# ---------------------------------------------
torch.cuda.empty_cache()
torch.cuda.ipc_collect()
print("✅ Pamięć GPU została wyczyszczona.")
